{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCCIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En este ejercicio se va a trabajar la analitica exploratoria (EDA) sobre un dataset de variables para la estimacion de fraude en operaciones bancarias.\n",
    "#### Para ello, vamos a empezar analizando el dataset disponible, el tipo de variables que lo componen y prepararlo para su posterior análisis y modelos de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline \n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iniciamos el proceso de análisis, estudiando los datos disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./FraudInputDatav2.csv', encoding = \"utf-16\") #leemos el fichero de datos, codificado en utf-16\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como podeis ver, se trata de un dataframe compuesto por 3 tipos de columnas, tenemos datos en coma flotante (float64), datos enteros (int64) y los datos tipo object son strings, es decir variables categóricas, las cuales posteriormente tendremos que linearizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vamos a linealizar las variables categoricas usando un one-hot-enconding\n",
    "#para eso el primer paso es encontrar todas las variables categoricas, seran aquellas que tengan el tipo object\n",
    "obj_df = df.select_dtypes(include=['object']).copy()\n",
    "obj_df.head()\n",
    "#como podeis ver el dataset tiene 8 variables categoricas. No linealizaremos FraudStr y sample ya que serán borradas posteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##realizamos la codificación\n",
    "df_encoding = pd.get_dummies(obj_df, columns=['pm','Channel','Product','sl','le','Routing'])\n",
    "df_encoding.head()\n",
    "#como podeis ver se ha creado una columna por cada valor de las variables categoricas.\n",
    "#Por ejemplo la variable payment ha sido convertida a 4 variables numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#obtenemos el resto de variables para concatenarlo en el df de trabajo\n",
    "num_df = df.select_dtypes(include=['int64','float64']).copy()\n",
    "df_num = pd.concat([df_encoding,num_df], axis=1).drop('ID',1).drop('random', 1).drop('sample', 1).drop('FraudStr', 1) #aprovecho para quitar columnas no necesarias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos a estudiar la variable Value de forma especial, ya que es una variable que habitualmente puede arrojar mucha informacion respecto al fraude de la operación. Aunque la variable está anonimizada como vereis es una muy buena medida discriminatoria.\n",
    "#### Podeis realizar estas mismas graficas sobre otros valores que creais de interés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins=160\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.hist(df_num.Value[df_num.Fraud==1],bins=bins,normed=True,alpha=0.8,label='Fraud',color='red')\n",
    "plt.hist(df_num.Value[df_num.Fraud==0],bins=bins,normed=True,alpha=0.8,label='Not Fraud',color='lightblue')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Valor')\n",
    "plt.ylabel('% de Registros')\n",
    "plt.title('Transacciones vs Valor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Como puede verse esta variable puede llegar a ser muy discriminante. Vamos a estudiar el resto de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estudiamos el resto de variables\n",
    "y=df_num.Fraud\n",
    "x=df_num.drop(['Fraud','Value'],axis=1) ## Quitamos la variable Value y la etiqueta Fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El primer punto es analizar la distribución de variables, tal como hemos visto en el apartado teórico. Lo que estamos buscando son variables cuya distribución difiera entre los casos de Fraude y NoFraude. Es decir, vamos a eliminar del dataset todas las variables cuya distribución sea muy similar entre ambos casos.\n",
    "#### Para facilitar la visualización de resultados, debido al alto número de variables, crearemos las gráficas en dos grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primer grupo de variables (0-60) La generación de estas gráficas lleva algo de tiempo de computo\n",
    "x_scaled=(x-x.min())/(x.max()-x.min()) \n",
    "sub_df1=pd.concat([y,x_scaled.iloc[:,0:10]],axis=1)\n",
    "sub_df2=pd.concat([y,x_scaled.iloc[:,10:20]],axis=1)\n",
    "sub_df3=pd.concat([y,x_scaled.iloc[:,20:30]],axis=1)\n",
    "sub_df4=pd.concat([y,x_scaled.iloc[:,30:40]],axis=1)\n",
    "sub_df5=pd.concat([y,x_scaled.iloc[:,40:50]],axis=1)\n",
    "sub_df6=pd.concat([y,x_scaled.iloc[:,50:60]],axis=1)\n",
    "sub_df11=pd.melt(sub_df1,id_vars=\"Fraud\",var_name=\"Variable\",value_name='Valor')\n",
    "sub_df22=pd.melt(sub_df2,id_vars=\"Fraud\",var_name=\"Variable\",value_name='Valor')\n",
    "sub_df33=pd.melt(sub_df3,id_vars=\"Fraud\",var_name=\"Variable\",value_name='Valor')\n",
    "sub_df44=pd.melt(sub_df4,id_vars=\"Fraud\",var_name=\"Variable\",value_name='Valor')\n",
    "sub_df55=pd.melt(sub_df5,id_vars=\"Fraud\",var_name=\"Variable\",value_name='Valor')\n",
    "sub_df66=pd.melt(sub_df6,id_vars=\"Fraud\",var_name=\"Variable\",value_name='Valor')\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.violinplot(x=\"Variable\",y=\"Valor\",hue=\"Fraud\",data=sub_df11, split=True)\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.violinplot(x=\"Variable\",y=\"Valor\",hue=\"Fraud\",data=sub_df22, split=True)\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.violinplot(x=\"Variable\",y=\"Valor\",hue=\"Fraud\",data=sub_df33, split=True)\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.violinplot(x=\"Variable\",y=\"Valor\",hue=\"Fraud\",data=sub_df44, split=True)\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.violinplot(x=\"Variable\",y=\"Valor\",hue=\"Fraud\",data=sub_df55, split=True)\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.violinplot(x=\"Variable\",y=\"Valor\",hue=\"Fraud\",data=sub_df66, split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segundo grupo de variables (60--) La generación de estas gráficas lleva algo de tiempo de computo\n",
    "x_scaled=(x-x.min())/(x.max()-x.min()) \n",
    "i=60\n",
    "sub_df1=pd.concat([y,x_scaled.iloc[:,i+0:i+10]],axis=1)\n",
    "sub_df2=pd.concat([y,x_scaled.iloc[:,i+10:i+20]],axis=1)\n",
    "sub_df3=pd.concat([y,x_scaled.iloc[:,i+20:i+30]],axis=1)\n",
    "sub_df4=pd.concat([y,x_scaled.iloc[:,i+30:i+40]],axis=1)\n",
    "sub_df5=pd.concat([y,x_scaled.iloc[:,i+40:i+50]],axis=1)\n",
    "\n",
    "sub_df11=pd.melt(sub_df1,id_vars=\"Fraud\",var_name=\"Variable\",value_name='Valor')\n",
    "sub_df22=pd.melt(sub_df2,id_vars=\"Fraud\",var_name=\"Variable\",value_name='Valor')\n",
    "sub_df33=pd.melt(sub_df3,id_vars=\"Fraud\",var_name=\"Variable\",value_name='Valor')\n",
    "sub_df44=pd.melt(sub_df4,id_vars=\"Fraud\",var_name=\"Variable\",value_name='Valor')\n",
    "sub_df55=pd.melt(sub_df5,id_vars=\"Fraud\",var_name=\"Variable\",value_name='Valor')\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.violinplot(x=\"Variable\",y=\"Valor\",hue=\"Fraud\",data=sub_df11, split=True)\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.violinplot(x=\"Variable\",y=\"Valor\",hue=\"Fraud\",data=sub_df22, split=True)\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.violinplot(x=\"Variable\",y=\"Valor\",hue=\"Fraud\",data=sub_df33, split=True)\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.violinplot(x=\"Variable\",y=\"Valor\",hue=\"Fraud\",data=sub_df44, split=True)\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.violinplot(x=\"Variable\",y=\"Valor\",hue=\"Fraud\",data=sub_df55, split=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#borramos columnas con una distribución uniforme entre fraude y no fraude\n",
    "df_features=df_num.drop(['Channel_BA','Channel_BB','Channel_BC','Channel_CA','Channel_D','Channel_E','Product_A','Product_B','Product_CA','Product_D','Product_E','Product_G','Product_Z','sl_JP','sl_NZ','Routing_0','Routing_aoldialup','Routing_aolpop','Routing_aolproxy','Routing_cache proxy','Routing_pop','Routing_regional proxy','Routing_satellite','ac','ab','N1','NCE','QualifiedGood','Risk_Email','Risk_CEP','Risk_Card','Risk_FistName','Risk_LastName','Risk_IP_Class','Risk_IP_Domain','Risk_IP_Carrier'],axis=1)\n",
    "df_features = pd.concat([df_features,df_num[['Value']]], axis=1) #incluimos el campo Value que fue extraido anteriormente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BALANCEO DE DATASET Y NORMALIZACIÓN DE DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En este punto hemos dejado el dataset con solamente aquellas variables que a-priori pueden aportar algún tipo de información respecto al fraude. Ahora vamos a analizar el dataset a nivel de tipo de información.\n",
    "#### Como vereis a continuación, un problema común que nos encontramos en la analítica operacional es que el dataset está desbalanceado, es decir existen muchas más muestras de una clase u etiqueta que de otra.\n",
    "#### Esto es especialmente importante cuando se detectan anomalías o situaciones como la de este ejercicio de detección de fraude. Existen muchas más transacciones legíticas que fraudulentas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classes = pd.value_counts(df_features['Fraud'], sort = True).sort_index()\n",
    "labels = 'Fraude', 'No Fraude'\n",
    "sizes = [count_classes[1]/(count_classes[1]+count_classes[0]), count_classes[0]/(count_classes[1]+count_classes[0])]\n",
    "explode = (0, 0.5,)  \n",
    "colors = ['red', 'blue']\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, explode=explode, colors=colors, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=45)\n",
    "ax1.axis('equal')  \n",
    "plt.title(\"Distribución del Dataset en clases etiquetadas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como podeis ver, el dataset está muy desbalanceado. Existen técnicas para mejorar el balanceo del mismo, posteriormente veremos algunas, pero lo más importante es que este tipo de situaciones deben ser tenidas en cuenta a la hora de analizar los resultados de los modelos que apliquemos. Lo veremos cuando analicemos los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El siguiente paso va a saer la normalización del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a analizar dentro del dataset aquellas columnas cuyo valor minimo sea menor que -1 y valor máximo mayor que 1.\n",
    "#Para ello vamos a apoyarnos en la funcion describe de un DataFrame que nos proporciona toda esta información y mas\n",
    "tt = df_features.describe().transpose()\n",
    "tt[(tt['max']>1) & (tt['min']< -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Como podeis ver existen 3 variables que es necesario normalizar, el resto son correctas\n",
    "columns_to_norm = ['Risk_Item','Risk_ProductType','Risk_Provider']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler() # se encarga de normalizar una variable entre -1 y 1\n",
    "df_features[columns_to_norm]=min_max_scaler.fit_transform(df_features[columns_to_norm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = df_features.describe().transpose()\n",
    "tt[(tt['max']>1) & (tt['min']< -1)]\n",
    "#Ya no quedan variables que normalizar, todas están en los rangos esperados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generamos una función de ayuda para el resto de módulos de cara a visualizar las matrices de confusión\n",
    "from sklearn.metrics import confusion_matrix, classification_report, auc, precision_recall_curve, roc_curve\n",
    "def plot_confusion_matrix(y_test, pred):\n",
    "    \n",
    "    y_test_legit = y_test.value_counts()[0]\n",
    "    y_test_fraud = y_test.value_counts()[1]\n",
    "    \n",
    "    cfn_matrix = confusion_matrix(y_test, pred)\n",
    "    cfn_norm_matrix = np.array([[1.0 / y_test_legit,1.0/y_test_legit],[1.0/y_test_fraud,1.0/y_test_fraud]])\n",
    "    norm_cfn_matrix = cfn_matrix * cfn_norm_matrix\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    sns.heatmap(cfn_matrix,cmap='coolwarm_r',linewidths=0.5,annot=True,ax=ax)\n",
    "    plt.title('Matriz de Confusión')\n",
    "    plt.ylabel('Categorias reales')\n",
    "    plt.xlabel('Categorias estimadas')\n",
    "\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    sns.heatmap(norm_cfn_matrix,cmap='coolwarm_r',linewidths=0.5,annot=True,ax=ax)\n",
    "\n",
    "    plt.title('Matriz de Confusión normalizada')\n",
    "    plt.ylabel('Categorias reales')\n",
    "    plt.xlabel('Categorias estimadas')\n",
    "    plt.show()\n",
    "    \n",
    "    print('---Report de clasificación---')\n",
    "    print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARACIÓN DE DATOS PARA APRENDIZAJE Y LANZAMIENTO DE MODELO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El primer paso es separar los datos en datos de entrenamiento y de test. En este ejercicio se usa una aproximación sencilla, separando los datos en 80% training, 20% test. Se pueden aplicar métodos más avanzados, incluyendo datos de validación posteriores o realizar una randominzación mayor de los datos a obtener, pero para este caso es suficiente esta aproximación.\n",
    "#### Asi mismo, es necesario separar los datos en variables y etiquetas (X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test = train_test_split(df_features, test_size=0.2, random_state=RANDOM_SEED)\n",
    "Y_train = X_train['Fraud']\n",
    "X_train = X_train.drop(['Fraud'], axis=1)\n",
    "Y_test = X_test['Fraud']\n",
    "X_test = X_test.drop(['Fraud'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos aplicar de inicio un modelo de regresión. Se trata de un modelo de regularización lineal que utiliza un Stochastic Gradient Descent (SGD). Es un modelo suficientemente robusto para una primera prueba aunque a futuro probaremos modelos más complejos.\n",
    "#### Para más información de este modelo consultar \n",
    "##### http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "sgd_clf=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
    "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
    "       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
    "       tol=None, verbose=0, warm_start=False)\n",
    "\n",
    "sgd_clf.fit(X_train, Y_train) \n",
    "Y_train_predicted=sgd_clf.predict(X_train)\n",
    "Y_test_predicted=sgd_clf.predict(X_test)\n",
    "\n",
    "plot_confusion_matrix(Y_test, Y_test_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A simple vista los resultados del modelo son bastante buenos. En el resultado medio vemos una precisión del 98% y un recall del 98% también. Recordemos que el término precisión nos indica la capacidad del clasificador para no etiquetar como positivo una muestra negativa. Es decir, la capacidad del modelo para no dar registros correctos como fraudulentos. El término recall nos indica la capacidad del clasificador para encontrar todas las muestra positivas.\n",
    "##### Pero si vemos en más detalle los resultados, vemos cómo en el caso de la clase 1, es decir Fraude, los resultados no son tan buenos. Si combinamos estos datos con la matriz de confusión (no normalizada) vemos cómo el modelo estima correctamente 630 registros sobre un total de aproximadamente 720, pero que etiqueta como fraudulentas 150 operaciones que en realidad son correctas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependiendo de cómo se operativice el modelo, esto puede ser un buen o mal resultado. Como pregunta abierta, ¿es mejor reducir el rendimiento en los registros correctos a  cambio de mejorar el rendimiento del modelo en operaciones fraudulentas? Veamos algunos ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DESBALANCEAR EL DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Una estrategia a seguir es reducir el número de muestras correctas, para intentar balancear el dataset. Al no ser un dataset especialmente grande es necesario tener cuidado con esta operación, ya que reducir drasticamente el número de muestras penalizará el modelo al no tener suficientes datos para su aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "Train_Data= pd.concat([X_train, Y_train], axis=1)\n",
    "X_1 =Train_Data[ Train_Data[\"Fraud\"]==1 ]\n",
    "X_0=Train_Data[Train_Data[\"Fraud\"]==0]\n",
    "\n",
    "X_0=shuffle(X_0,random_state=42).reset_index(drop=True)\n",
    "X_1=shuffle(X_1,random_state=42).reset_index(drop=True)\n",
    "\n",
    "ALPHA=1 #sobre esta variable podeis jugar para cambiar la distribución del dataset\n",
    "\n",
    "X_0=X_0.iloc[:round(len(X_1)*ALPHA),:]\n",
    "data_d=pd.concat([X_1, X_0])\n",
    "\n",
    "count_classes = pd.value_counts(data_d['Fraud'], sort = True).sort_index()\n",
    "labels = 'Fraude', 'No Fraude'\n",
    "sizes = [count_classes[1]/(count_classes[1]+count_classes[0]), count_classes[0]/(count_classes[1]+count_classes[0])]\n",
    "explode = (0, 0.05,)\n",
    "colors = ['red', 'blue']\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, explode=explode, colors=colors, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=45)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.title(\"Distribución del dataset en clases\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_d=data_d['Fraud']\n",
    "X_d=data_d.drop(['Fraud'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf_d=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
    "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
    "       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
    "       tol=None, verbose=0, warm_start=False)\n",
    "\n",
    "sgd_clf_d.fit(X_d, Y_d) \n",
    "\n",
    "\n",
    "Y_test_predicted=sgd_clf_d.predict(X_test)\n",
    "\n",
    "plot_confusion_matrix(Y_test, Y_test_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Si observamos este escenario, donde hemos reducido el número de muestas de entrenamiento a un 50% correctas, 50% fraudulentas, por un lado el número de falsos negativos en la clase fraude ha disminuido de 95 a 20, mientras que el número de falsos positivos, entendidos como el número de registros correctos que han sido detectados como fraudulentos ha aumentado de 150 a 950 aproximadamente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para finalizar, vamos a ejecutar el mismo dataset balanceado con un modelo algo más complejo y potente como es el RandomForest, y analicemos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "rf =RandomForestClassifier(n_estimators=100, max_depth=None, random_state=0, n_jobs=-1)\n",
    "rf.fit(X_d, Y_d) \n",
    "Y_test_predicted=rf.predict(X_test)\n",
    "\n",
    "plot_confusion_matrix(Y_test, Y_test_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En términos generales se trata de un modelo que arroja mejores resultados, aunque si miramos la matriz de confusión ha aumentado de 20 a 38 los falsos negativos, y ha disminuido de 950 a 350 aproximadamente en los falsos positivos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
